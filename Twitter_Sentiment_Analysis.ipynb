{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Twitter Sentiment Analysis.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPEB7E/VgrArqRpQfBsNzn4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jasonyang429/Twitter-Sentiment-Analysis-Simple/blob/main/Twitter_Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAHv-41T5Epc",
        "outputId": "4625408c-f650-4ca3-f2b0-349b8415b194"
      },
      "source": [
        "# Importing all the neccessary packages and libraries\r\n",
        "import nltk\r\n",
        "import tensorflow as tf\r\n",
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "from nltk.stem.porter import PorterStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.tag import pos_tag\r\n",
        "import re\r\n",
        "from nltk.corpus import twitter_samples\r\n",
        "import string\r\n",
        "from sklearn.utils import shuffle\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "\r\n",
        "nltk.download('twitter_samples')\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYaz3g1N5jkA"
      },
      "source": [
        "Here, I combined the positive tweets and negative tweets and created a labels for each of them.\r\n",
        "\r\n",
        "\"1\" stands for Positive sentiment tweets and \"0\" stands for Negative sentiment tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_jeZLwVd5vfn"
      },
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\r\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\r\n",
        "\r\n",
        "stop_words = set(stopwords.words('english'))\r\n",
        "\r\n",
        "tweets = positive_tweets + negative_tweets\r\n",
        "\r\n",
        "Y = np.array([1]*(len(tweets)//2) + [0]*(len(tweets)//2))"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ix4s963D9mQz",
        "outputId": "29dedd22-8a77-43c9-abb4-d37dc29abcd3"
      },
      "source": [
        "print(tweets[0])\r\n",
        "print(Y[0])"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSnzoROKp2_5"
      },
      "source": [
        "This cell below I implemented a method to preprocess the tweets in to a tokenized version to pass into the network later.\r\n",
        "\r\n",
        "There are 2 concepts applied here which are stemming and lemmatizing.\r\n",
        "\r\n",
        "*    Stemming is to reduce the word into its base forms, like 'eating' -> 'eat'\r\n",
        "*    Lemmatizing is to change the word in to another word form without changing its meaning. For example, 'better' -> 'good' <sup>[3]</sup>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxDU8qFWzp8e"
      },
      "source": [
        "def preprocess_texts(tweet):\r\n",
        "  # Lower case the sentence\r\n",
        "  tweet = tweet.lower()\r\n",
        "\r\n",
        "  # Substitute all URL links starting with http, https, www with '' \r\n",
        "  tweet = re.sub(r'http\\S+|https\\S+|www\\S+', '', tweet)\r\n",
        "\r\n",
        "  # Substitute all @ and # to removes taggings with ''\r\n",
        "  tweet = re.sub(r'\\@\\w+|\\#', '', tweet)\r\n",
        "\r\n",
        "  # Can refer to reference no. 5\r\n",
        "  # maketrans() functions: \r\n",
        "  #   1st argument is mapped to 2nd arguments for substitution \r\n",
        "  #   all the ''(1st argument) in tweets are substituted with ''(2nd argument)\r\n",
        "  #   3rd argument is the characters to be removed from the tweets\r\n",
        "  #   so the function is to replace '' with '' and remove all string.punctuation\r\n",
        "  tweet = tweet.translate(str.maketrans('', '', string.punctuation))\r\n",
        "\r\n",
        "  # Tokenize the tweets with nltk\r\n",
        "  tweet_tokens = word_tokenize(tweet)\r\n",
        "\r\n",
        "  # Remove the stopwords from tweets\r\n",
        "  tweet_tokens_without_stopwords = [token for token in tweet_tokens if not token in stop_words]\r\n",
        "\r\n",
        "  # Stemming the words \r\n",
        "  stemmer = PorterStemmer()\r\n",
        "\r\n",
        "  stemmed_tokens = [stemmer.stem(token) for token in tweet_tokens_without_stopwords]\r\n",
        "  \r\n",
        "  ### Can uncomment this part for lemmatizing the tweets\r\n",
        "  ### Not encouraged to do both stemming and lemmatizing\r\n",
        "  # lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "  # lemmatized_tokens = []\r\n",
        "  # for token, tag in pos_tag(stemmed_tokens):\r\n",
        "  #   if tag.startswith('NN'):\r\n",
        "  #     tag = 'n'\r\n",
        "  #   elif tag.startswith('VB'):\r\n",
        "  #     tag = 'vb'\r\n",
        "  #   else:\r\n",
        "  #     tag = 'a'\r\n",
        "  # lemmatized_tokens = [lemmatizer.lemmatize(token, pos='a') for token in tweet_tokens_without_stopwords]\r\n",
        "\r\n",
        "  # return lemmatized_tokens\r\n",
        "  return stemmed_tokens\r\n"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppLIJebl6O0i"
      },
      "source": [
        "Then, proceed to tokenize the workds and padding it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phnyiiV_95Bp"
      },
      "source": [
        "# Set the maximum number of words stored in the dictionary\r\n",
        "VOCAB_SIZE = 10000\r\n",
        "\r\n",
        "# Preprocess all tweets\r\n",
        "tweets = [preprocess_texts(tweet) for tweet in tweets]\r\n",
        "\r\n",
        "# Define the dictionary to store the vocabularies\r\n",
        "# Maximum number of words stored is VOCAB_SIZE\r\n",
        "tokenizers = Tokenizer(num_words=VOCAB_SIZE)\r\n",
        "tokenizers.fit_on_texts(tweets)\r\n",
        "\r\n",
        "# Vectorise all words into numbers\r\n",
        "X = tokenizers.texts_to_sequences(tweets)\r\n",
        "X = pad_sequences(X, padding='post')\r\n",
        "\r\n",
        "# Changing the labels from lists to categorical vector\r\n",
        "Y = tf.keras.utils.to_categorical(Y)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC7lth9YerS9"
      },
      "source": [
        "Debugged for 2 days because I forgot to convert Y labels into categorical matrix, and I used categorical crossentropy as loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DV5DrqIhzzGI",
        "outputId": "ade6e48e-2589-4171-a1b9-1eb8b9df9d47"
      },
      "source": [
        "print(tweets[20])\r\n",
        "print(X[20])\r\n",
        "print(Y[20])"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bc', 'realli', 'dont', 'feel', 'like', 'read']\n",
            "[158  29  14  31   5 168   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0]\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06bIDuH1z8Bg"
      },
      "source": [
        "# Shuffle the X, Y and tweets together\r\n",
        "# random_state=0 is to ensure the same results will be obtained everytime\r\n",
        "# random_state is similar to seed \r\n",
        "X, Y, tweets = shuffle(X, Y, tweets, random_state=0)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFL193xo6vNl",
        "outputId": "ade6e48e-2589-4171-a1b9-1eb8b9df9d47"
      },
      "source": [
        "print(tweets[20])\r\n",
        "print(X[20])\r\n",
        "print(Y[20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['bc', 'realli', 'dont', 'feel', 'like', 'read']\n",
            "[158  29  14  31   5 168   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0]\n",
            "[0. 1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w3uya7p1bgkr",
        "outputId": "5960d0ab-2250-4dec-9e8e-547c79fe4dba"
      },
      "source": [
        "# Fix the input dimensions and length for the embedding layer of the model\r\n",
        "INPUT_DIMS = VOCAB_SIZE\r\n",
        "INPUT_LENGTH = X.shape[1]\r\n",
        "\r\n",
        "print(INPUT_LENGTH)\r\n",
        "print(INPUT_DIMS)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "27\n",
            "10000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At30NnhM7DE8"
      },
      "source": [
        "Here, I created the model with LSTM. Feel free to use other such as GRU or Conv1D"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l1aeExX0M-d",
        "outputId": "6d82e394-f9e5-43ae-a1ed-bbea923cd046"
      },
      "source": [
        "# Specify the embedding dimensions for the model\r\n",
        "EMBEDDING_DIMS = 64\r\n",
        "\r\n",
        "# The model\r\n",
        "# Embedding layer for getting embedding of words\r\n",
        "#   means that each word has 64 meaning representations\r\n",
        "# LSTM = Long Short-Term Memory gates\r\n",
        "# Dropout\r\n",
        "#   for regularization purpose\r\n",
        "# Dense\r\n",
        "#   Fully connected layer with 1 neurons for predictions\r\n",
        "model = tf.keras.Sequential([\r\n",
        "          tf.keras.layers.Embedding(INPUT_DIMS, EMBEDDING_DIMS, input_length=INPUT_LENGTH),\r\n",
        "          tf.keras.layers.LSTM(16, recurrent_dropout=0.3, dropout=0.3, recurrent_regularizer='l2'),\r\n",
        "          tf.keras.layers.Dropout(0.4),\r\n",
        "          tf.keras.layers.Dense(2, activation='softmax')\r\n",
        "])\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_16 (Embedding)     (None, 27, 64)            640000    \n",
            "_________________________________________________________________\n",
            "lstm_16 (LSTM)               (None, 16)                5184      \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 16)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 2)                 34        \n",
            "=================================================================\n",
            "Total params: 645,218\n",
            "Trainable params: 645,218\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FotSp_d8cr59"
      },
      "source": [
        "# Split into training, evaluation and test sets\r\n",
        "TRAIN_SPLIT = int(0.9*len(X))\r\n",
        "\r\n",
        "x_test = X[TRAIN_SPLIT:]\r\n",
        "x_train = X[:TRAIN_SPLIT]\r\n",
        "\r\n",
        "y_test = Y[TRAIN_SPLIT:]\r\n",
        "y_train = Y[:TRAIN_SPLIT]\r\n",
        "\r\n",
        "EVAL_SPLIT = int(0.9 * len(x_train))\r\n",
        "\r\n",
        "x_eval = x_train[EVAL_SPLIT:]\r\n",
        "x_train = x_train[:EVAL_SPLIT]\r\n",
        "\r\n",
        "y_eval = y_train[EVAL_SPLIT:]\r\n",
        "y_train = y_train[:EVAL_SPLIT]\r\n",
        "\r\n"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48I8wls2cG6I",
        "outputId": "83e91577-c623-4e9e-fae2-eb1b2c729502"
      },
      "source": [
        "# Training the model\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\r\n",
        "\r\n",
        "model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_eval, y_eval))"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "254/254 [==============================] - 32s 116ms/step - loss: 0.7822 - accuracy: 0.5017 - val_loss: 0.7006 - val_accuracy: 0.4800\n",
            "Epoch 2/10\n",
            "254/254 [==============================] - 30s 116ms/step - loss: 0.6960 - accuracy: 0.5121 - val_loss: 0.5905 - val_accuracy: 0.7056\n",
            "Epoch 3/10\n",
            "254/254 [==============================] - 29s 116ms/step - loss: 0.4964 - accuracy: 0.7905 - val_loss: 0.4992 - val_accuracy: 0.7600\n",
            "Epoch 4/10\n",
            "254/254 [==============================] - 29s 115ms/step - loss: 0.3356 - accuracy: 0.8776 - val_loss: 0.5391 - val_accuracy: 0.7533\n",
            "Epoch 5/10\n",
            "254/254 [==============================] - 29s 116ms/step - loss: 0.2806 - accuracy: 0.9037 - val_loss: 0.6041 - val_accuracy: 0.7600\n",
            "Epoch 6/10\n",
            "254/254 [==============================] - 29s 116ms/step - loss: 0.2292 - accuracy: 0.9281 - val_loss: 0.6789 - val_accuracy: 0.7522\n",
            "Epoch 7/10\n",
            "254/254 [==============================] - 29s 115ms/step - loss: 0.1957 - accuracy: 0.9401 - val_loss: 0.6665 - val_accuracy: 0.7456\n",
            "Epoch 8/10\n",
            "254/254 [==============================] - 30s 116ms/step - loss: 0.1856 - accuracy: 0.9425 - val_loss: 0.6487 - val_accuracy: 0.7556\n",
            "Epoch 9/10\n",
            "254/254 [==============================] - 29s 113ms/step - loss: 0.1710 - accuracy: 0.9468 - val_loss: 0.7027 - val_accuracy: 0.7422\n",
            "Epoch 10/10\n",
            "254/254 [==============================] - 29s 114ms/step - loss: 0.1552 - accuracy: 0.9537 - val_loss: 0.8478 - val_accuracy: 0.7456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7f6bd5a550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqyWOIl-78-B"
      },
      "source": [
        "The model is overfitting to the training sets, possible improvements could be : \r\n",
        "\r\n",
        "\r\n",
        "*   Adding more regularization\r\n",
        "*   Reduce model complexity\r\n",
        "*   Using higher dropout rates\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rhh9HFkedBRM",
        "outputId": "ce9b35b9-c856-40fa-8f30-2fb2a159f1ba"
      },
      "source": [
        "# Evaluating the model\r\n",
        "model.evaluate(x_test, y_test)"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 9ms/step - loss: 0.7941 - accuracy: 0.7600\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.7940672039985657, 0.7599999904632568]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiJ4IuIc727f"
      },
      "source": [
        "The cell below is to predict any sentences. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhQr5H3LeP48",
        "outputId": "8d7d2afc-59f2-4343-9f52-c9662964cc04"
      },
      "source": [
        "your_text = ['I am so idiot']\r\n",
        "your_text = [preprocess_texts(text) for text in your_text]\r\n",
        "your_text = tokenizers.texts_to_sequences(your_text)\r\n",
        "your_text = pad_sequences(your_text, maxlen=X.shape[1], padding='post')\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def get_sentiment(text):\r\n",
        "  sentiment = model.predict(text, batch_size=1, verbose=2)[0]\r\n",
        "  print(sentiment)\r\n",
        "\r\n",
        "  if(np.argmax(sentiment) == 0):\r\n",
        "    print(\"Negative sentiment\")\r\n",
        "  else:\r\n",
        "    print(\"Positive sentiment\")\r\n",
        "\r\n",
        "get_sentiment(your_text)\r\n"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1/1 - 0s\n",
            "[0.867867   0.13213296]\n",
            "Negative sentiment\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2u-Nh6cilPS"
      },
      "source": [
        "References : \r\n",
        "\r\n",
        "1.   [Keras LSTM Twitter Sentiment Analsysis](https://www.kaggle.com/vandalko/keras-lstm-twitter-sentiment-analysis)\r\n",
        "2.   [LSTM Sentiment Analysis | Keras](https://www.kaggle.com/ngyptr/lstm-sentiment-analysis-keras)\r\n",
        "3. [Building a Twitter Sentiment Analysis in Python](https://www.pluralsight.com/guides/building-a-twitter-sentiment-analysis-in-python)\r\n",
        "4. [How To Perform Sentiment Analysis in Python 3 Using the Natural Language Toolkit (NLTK)](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)\r\n",
        "5. [Python String maketrans() Method](https://www.w3schools.com/python/ref_string_maketrans.asp)\r\n",
        "\r\n",
        "\r\n"
      ]
    }
  ]
}